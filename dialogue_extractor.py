#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
å¯¹è¯æå–å™¨ï¼šä»å°è¯´æ–‡æœ¬ä¸­æå–è§’è‰²å¯¹è¯
æ•´åˆç‰ˆæœ¬ï¼ŒåŒ…å«æ‰€æœ‰æ ¸å¿ƒåŠŸèƒ½
"""

import os
import json
import time
import logging
import threading
from concurrent.futures import ThreadPoolExecutor, as_completed
from queue import Queue
from typing import Dict, List, Any, Optional, Set, Union
from dataclasses import dataclass
from pathlib import Path

import tiktoken
from tqdm import tqdm
from openai import OpenAI
from dotenv import load_dotenv, find_dotenv

from config import Config

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv(find_dotenv())

# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    level=getattr(logging, Config.LOG_LEVEL),
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(Config.LOG_FILE, encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

@dataclass
class DialogueItem:
    """å¯¹è¯æ•°æ®ç±»"""
    role: str
    dialogue: str
    
    def to_dict(self) -> Dict[str, str]:
        return {"role": self.role, "dialogue": self.dialogue}
    
    def __hash__(self) -> int:
        """ç”¨äºå»é‡çš„å“ˆå¸Œå€¼"""
        return hash((self.role.strip().lower(), self.dialogue.strip().lower()))

@dataclass
class ChunkDialogueItem:
    """å¸¦chunk-idçš„å¯¹è¯æ•°æ®ç±»"""
    chunk_id: int
    dialogue_index: int
    role: str
    dialogue: str
    chunk_text: Optional[str] = None
    
    def to_dict(self, include_chunk_text: bool = False) -> Dict[str, Any]:
        """è½¬æ¢ä¸ºå­—å…¸æ ¼å¼"""
        result = {
            "chunk_id": self.chunk_id,
            "dialogue_index": self.dialogue_index,
            "role": self.role,
            "dialogue": self.dialogue
        }
        if include_chunk_text and self.chunk_text:
            result["chunk_text"] = self.chunk_text
        return result
    
    def to_dialogue_item(self) -> DialogueItem:
        """è½¬æ¢ä¸ºæ™®é€šå¯¹è¯é¡¹ï¼ˆå‘åå…¼å®¹ï¼‰"""
        return DialogueItem(role=self.role, dialogue=self.dialogue)
    
    def __hash__(self) -> int:
        """ç”¨äºå»é‡çš„å“ˆå¸Œå€¼"""
        return hash((self.role.strip().lower(), self.dialogue.strip().lower()))
    
    def __eq__(self, other) -> bool:
        """ç”¨äºå»é‡çš„ç­‰ä»·æ¯”è¾ƒ"""
        if isinstance(other, ChunkDialogueItem):
            return self.role == other.role and self.dialogue == other.dialogue
        elif isinstance(other, DialogueItem):
            return self.role == other.role and self.dialogue == other.dialogue
        return False

@dataclass
class WorkItem:
    """å·¥ä½œå•å…ƒæ•°æ®ç±»"""
    index: int
    chunk_id: int
    chunk: str
    system_prompt: str
    
class ThreadSafeDialogueExtractor:
    """çº¿ç¨‹å®‰å…¨çš„å¯¹è¯æå–å™¨"""
    
    def __init__(self, extractor: 'DialogueExtractor', include_chunk_id: bool = True):
        self.extractor = extractor
        self.lock = threading.Lock()
        self.seen_dialogues = set()
        self.total_dialogues = 0
        self.processed_chunks = 0
        self.errors = []
        self.include_chunk_id = include_chunk_id
        
    def process_chunk(self, work_item: WorkItem) -> List[ChunkDialogueItem]:
        """å¤„ç†å•ä¸ªæ–‡æœ¬å—"""
        try:
            # è°ƒç”¨APIæå–å¯¹è¯
            response = self.extractor._call_api_with_retry(
                work_item.system_prompt, 
                work_item.chunk
            )
            dialogues = self.extractor._parse_and_validate_response(response)
            
            # çº¿ç¨‹å®‰å…¨çš„å»é‡å’Œè½¬æ¢
            with self.lock:
                unique_dialogues = []
                for dialogue_index, dialogue in enumerate(dialogues):
                    if dialogue not in self.seen_dialogues:
                        self.seen_dialogues.add(dialogue)
                        
                        if self.include_chunk_id:
                            # åˆ›å»ºå¸¦chunk-idçš„å¯¹è¯é¡¹
                            chunk_dialogue = ChunkDialogueItem(
                                chunk_id=work_item.chunk_id,
                                dialogue_index=dialogue_index,
                                role=dialogue.role,
                                dialogue=dialogue.dialogue,
                                chunk_text=work_item.chunk if self.extractor.save_chunk_text else None
                            )
                            unique_dialogues.append(chunk_dialogue)
                        else:
                            # ä¿æŒå‘åå…¼å®¹
                            unique_dialogues.append(dialogue)
                
                self.total_dialogues += len(unique_dialogues)
                self.processed_chunks += 1
                
                return unique_dialogues
                
        except Exception as e:
            with self.lock:
                self.errors.append(f"å¤„ç†ç¬¬ {work_item.index + 1} ä¸ªå—æ—¶å‘ç”Ÿé”™è¯¯: {e}")
            logger.error(f"å¤„ç†ç¬¬ {work_item.index + 1} ä¸ªå—æ—¶å‘ç”Ÿé”™è¯¯: {e}")
            return []

class DialogueExtractor:
    """å¯¹è¯æå–å™¨ä¸»ç±»"""
    
    def __init__(self, schema: Optional[Dict] = None, platform: Optional[str] = None, max_workers: Optional[int] = None, 
                 include_chunk_id: Optional[bool] = None, save_chunk_text: Optional[bool] = None):
        """
        åˆå§‹åŒ–å¯¹è¯æå–å™¨
        
        Args:
            schema: è‡ªå®šä¹‰æå–æ¨¡å¼ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é»˜è®¤æ¨¡å¼
            platform: æŒ‡å®šä½¿ç”¨çš„å¹³å°ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨ç¯å¢ƒå˜é‡ä¸­çš„é…ç½®
            max_workers: æœ€å¤§å¹¶å‘çº¿ç¨‹æ•°ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é…ç½®ä¸­çš„é»˜è®¤å€¼
            include_chunk_id: æ˜¯å¦åœ¨è¾“å‡ºä¸­åŒ…å«chunk-idï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é…ç½®ä¸­çš„é»˜è®¤å€¼
            save_chunk_text: æ˜¯å¦ä¿å­˜åŸå§‹chunkæ–‡æœ¬ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é…ç½®ä¸­çš„é»˜è®¤å€¼
        """
        # è®¾ç½®å¹³å°
        if platform:
            Config.set_platform(platform)
        
        # éªŒè¯é…ç½®
        config_errors = Config.validate_config()
        if config_errors:
            raise ValueError(f"é…ç½®é”™è¯¯: {'; '.join(config_errors)}")
        
        # è·å–å½“å‰å¹³å°é…ç½®
        platform_config = Config.get_current_platform_config()
        self.platform = platform_config['platform']
        self.model_name = platform_config['model_name']
        
        self.schema = schema or Config.DEFAULT_SCHEMA
        self.client = OpenAI(
            api_key=platform_config['api_key'],
            base_url=platform_config['base_url']
        )
        self.encoder = tiktoken.get_encoding(Config.ENCODING)
        
        # çº¿ç¨‹é…ç½®
        self.max_workers = max_workers or Config.MAX_WORKERS
        
        # Chunk-id é…ç½®
        self.include_chunk_id = include_chunk_id if include_chunk_id is not None else getattr(Config, 'INCLUDE_CHUNK_ID', True)
        self.save_chunk_text = save_chunk_text if save_chunk_text is not None else getattr(Config, 'SAVE_CHUNK_TEXT', False)
        
        # ç”¨äºå»é‡çš„é›†åˆï¼ˆä»…åœ¨å•çº¿ç¨‹æ¨¡å¼ä¸‹ä½¿ç”¨ï¼‰
        self.seen_dialogues: Set[DialogueItem] = set()
        
        # åˆ›å»ºç¼“å­˜ç›®å½•
        Path(Config.CACHE_DIR).mkdir(exist_ok=True)
        
        logger.info(f"å¯¹è¯æå–å™¨åˆå§‹åŒ–å®Œæˆ - å¹³å°: {self.platform} ({platform_config['description']})")
        logger.info(f"ä½¿ç”¨æ¨¡å‹: {self.model_name}")
        logger.info(f"æœ€å¤§å¹¶å‘çº¿ç¨‹æ•°: {self.max_workers}")
    
    def _generate_system_prompt(self) -> str:
        """ç”Ÿæˆç³»ç»Ÿæç¤º"""
        attributes = self.schema['attributes']
        attributes_str = ',\n    '.join([
            f"{attr['name']}: {attr['type']} // {attr['description']}"
            for attr in attributes
        ])
        
        typescript = Config.get_typescript_template().format(
            task_description=self.schema['task_description'],
            attributes=attributes_str
        )
        
        example_input = self.schema['example'][0]['text']
        example_output = json.dumps(
            self.schema['example'][0]['script'], 
            indent=4, 
            ensure_ascii=False
        )
        
        return Config.get_system_prompt_template().format(
            TypeScript=typescript,
            Input=example_input,
            Output=example_output
        )
    
    def _read_text_file(self, file_path: str) -> str:
        """è¯»å–æ–‡æœ¬æ–‡ä»¶"""
        try:
            with open(file_path, 'r', encoding='utf-8') as file:
                return file.read()
        except Exception as e:
            logger.error(f"è¯»å–æ–‡ä»¶å¤±è´¥ {file_path}: {e}")
            raise
    
    def _chunk_text(self, text: str) -> List[str]:
        """
        ä¼˜åŒ–çš„æ–‡æœ¬åˆ†å—ç®—æ³•
        å‡å°‘é‡å¤å¯¹è¯ï¼Œæé«˜åˆ†å—è´¨é‡
        """
        chunks = []
        lines = text.split('\n')
        
        current_chunk = ""
        current_tokens = 0
        
        # é¢„å¤„ç†ï¼šæ¸…ç†ç©ºè¡Œå’Œå¤šä½™ç©ºæ ¼
        cleaned_lines = []
        for line in lines:
            cleaned = line.strip()
            if cleaned:  # åªä¿ç•™éç©ºè¡Œ
                cleaned_lines.append(cleaned)
        
        i = 0
        while i < len(cleaned_lines):
            line = cleaned_lines[i]
            line_tokens = len(self.encoder.encode(line))
            
            # å¦‚æœå•è¡Œè¶…è¿‡é™åˆ¶ï¼Œå¼ºåˆ¶åˆ†å‰²
            if line_tokens > Config.MAX_TOKEN_LEN:
                if current_chunk:
                    chunks.append(current_chunk)
                    current_chunk = ""
                    current_tokens = 0
                
                # æŒ‰å¥å­åˆ†å‰²é•¿è¡Œ
                sentences = self._split_long_line(line)
                for sentence in sentences:
                    chunks.append(sentence)
                i += 1
                continue
            
            # å¦‚æœæ·»åŠ å½“å‰è¡Œä¸ä¼šè¶…è¿‡é™åˆ¶
            if current_tokens + line_tokens + 1 <= Config.MAX_TOKEN_LEN:
                current_chunk += line + "\n"
                current_tokens += line_tokens + 1
                i += 1
            else:
                # å½“å‰å—å·²æ»¡ï¼Œä¿å­˜å¹¶å¼€å§‹æ–°å—
                if current_chunk:
                    chunks.append(current_chunk.rstrip())
                
                # ä¸ºä¿æŒä¸Šä¸‹æ–‡ï¼Œæ·»åŠ é‡å å†…å®¹
                overlap_lines = []
                temp_tokens = 0
                j = max(0, i - 5)  # æœ€å¤šå›æº¯5è¡Œ
                
                while j < i and temp_tokens < Config.COVER_CONTENT:
                    line_j = cleaned_lines[j]
                    tokens_j = len(self.encoder.encode(line_j))
                    if temp_tokens + tokens_j <= Config.COVER_CONTENT:
                        overlap_lines.append(line_j)
                        temp_tokens += tokens_j
                    j += 1
                
                current_chunk = "\n".join(overlap_lines) + "\n"
                current_tokens = temp_tokens
        
        # æ·»åŠ æœ€åä¸€ä¸ªå—
        if current_chunk:
            chunks.append(current_chunk.rstrip())
        
        logger.info(f"æ–‡æœ¬åˆ†å—å®Œæˆï¼šå…± {len(chunks)} ä¸ªå—")
        return chunks
    
    def _split_long_line(self, long_line: str) -> List[str]:
        """å°†é•¿è¡ŒæŒ‰å¥å­åˆ†å‰²"""
        import re
        
        # æŒ‰å¥å·ã€é—®å·ã€æ„Ÿå¹å·åˆ†å‰²
        sentences = re.split(r'([ã€‚ï¼ï¼Ÿ])', long_line)
        
        chunks = []
        current_chunk = ""
        
        for i in range(0, len(sentences), 2):
            if i + 1 < len(sentences):
                sentence = sentences[i] + sentences[i + 1]
            else:
                sentence = sentences[i]
            
            tokens = len(self.encoder.encode(current_chunk + sentence))
            if tokens <= Config.MAX_TOKEN_LEN:
                current_chunk += sentence
            else:
                if current_chunk:
                    chunks.append(current_chunk)
                current_chunk = sentence
        
        if current_chunk:
            chunks.append(current_chunk)
        
        return chunks
    
    def _call_api_with_retry(self, system_prompt: str, user_prompt: str) -> str:
        """å¸¦é‡è¯•æœºåˆ¶çš„APIè°ƒç”¨"""
        for attempt in range(Config.MAX_RETRIES):
            try:
                response = self.client.chat.completions.create(
                    model=self.model_name,
                    messages=[
                        {"role": "system", "content": system_prompt},
                        {"role": "user", "content": user_prompt}
                    ],
                    temperature=Config.TEMPERATURE,
                    stream=False
                )
                return response.choices[0].message.content
                
            except Exception as e:
                logger.warning(f"APIè°ƒç”¨å¤±è´¥ (å°è¯• {attempt + 1}/{Config.MAX_RETRIES}): {e}")
                if attempt < Config.MAX_RETRIES - 1:
                    time.sleep(Config.RETRY_DELAY * (attempt + 1))
                else:
                    logger.error("APIè°ƒç”¨é‡è¯•æ¬¡æ•°å·²ç”¨å®Œ")
                    raise
    
    def _parse_and_validate_response(self, response: str) -> List[DialogueItem]:
        """è§£æå¹¶éªŒè¯APIå“åº”"""
        try:
            data = json.loads(response)
            if not isinstance(data, list):
                logger.warning("å“åº”ä¸æ˜¯åˆ—è¡¨æ ¼å¼ï¼Œå°è¯•è½¬æ¢")
                if isinstance(data, dict) and 'script' in data:
                    data = data['script']
                else:
                    return []
            
            dialogues = []
            for item in data:
                if isinstance(item, dict) and 'role' in item and 'dialogue' in item:
                    dialogue = DialogueItem(
                        role=str(item['role']).strip(),
                        dialogue=str(item['dialogue']).strip()
                    )
                    
                    # éªŒè¯å†…å®¹ä¸ä¸ºç©º
                    if dialogue.role and dialogue.dialogue:
                        dialogues.append(dialogue)
                    else:
                        logger.warning(f"è·³è¿‡ç©ºå¯¹è¯é¡¹: {item}")
                else:
                    logger.warning(f"è·³è¿‡æ— æ•ˆå¯¹è¯é¡¹: {item}")
            
            return dialogues
            
        except json.JSONDecodeError as e:
            logger.error(f"JSONè§£æå¤±è´¥: {e}")
            logger.debug(f"åŸå§‹å“åº”: {response}")
            return []
    
    def _remove_duplicates(self, dialogues: List[DialogueItem]) -> List[DialogueItem]:
        """ç§»é™¤é‡å¤å¯¹è¯"""
        unique_dialogues = []
        for dialogue in dialogues:
            if dialogue not in self.seen_dialogues:
                unique_dialogues.append(dialogue)
                self.seen_dialogues.add(dialogue)
        
        removed_count = len(dialogues) - len(unique_dialogues)
        if removed_count > 0:
            logger.info(f"ç§»é™¤äº† {removed_count} ä¸ªé‡å¤å¯¹è¯")
        
        return unique_dialogues
    
    def _save_progress(self, file_path: str, processed_chunks: int, total_chunks: int):
        """ä¿å­˜è¿›åº¦ä¿¡æ¯"""
        progress_file = os.path.join(Config.CACHE_DIR, Config.PROGRESS_FILE)
        progress_data = {
            'file_path': file_path,
            'processed_chunks': processed_chunks,
            'total_chunks': total_chunks,
            'timestamp': time.time()
        }
        
        try:
            with open(progress_file, 'w', encoding='utf-8') as f:
                json.dump(progress_data, f, ensure_ascii=False, indent=2)
        except Exception as e:
            logger.warning(f"ä¿å­˜è¿›åº¦å¤±è´¥: {e}")
    
    def _load_progress(self, file_path: str) -> Optional[int]:
        """åŠ è½½è¿›åº¦ä¿¡æ¯"""
        progress_file = os.path.join(Config.CACHE_DIR, Config.PROGRESS_FILE)
        
        try:
            if os.path.exists(progress_file):
                with open(progress_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    if data.get('file_path') == file_path:
                        return data.get('processed_chunks', 0)
        except Exception as e:
            logger.warning(f"åŠ è½½è¿›åº¦å¤±è´¥: {e}")
        
        return None
    
    def extract_dialogues(self, file_path: str, output_file: Optional[str] = None) -> str:
        """
        ä»æ–‡æœ¬æ–‡ä»¶ä¸­æå–å¯¹è¯
        
        Args:
            file_path: è¾“å…¥æ–‡æœ¬æ–‡ä»¶è·¯å¾„
            output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼Œå¦‚æœä¸ºNoneåˆ™è‡ªåŠ¨ç”Ÿæˆ
        
        Returns:
            è¾“å‡ºæ–‡ä»¶è·¯å¾„
        """
        logger.info(f"å¼€å§‹å¤„ç†æ–‡ä»¶: {file_path}")
        
        # è¯»å–æ–‡æœ¬æ–‡ä»¶
        text = self._read_text_file(file_path)
        
        # æ–‡æœ¬åˆ†å—
        chunks = self._chunk_text(text)
        
        # æ£€æŸ¥æ˜¯å¦æœ‰è¿›åº¦å¯ä»¥æ¢å¤
        processed_chunks = self._load_progress(file_path) or 0
        if processed_chunks > 0:
            logger.info(f"æ¢å¤è¿›åº¦ï¼šå·²å¤„ç† {processed_chunks}/{len(chunks)} ä¸ªå—")
        
        # ç”Ÿæˆç³»ç»Ÿæç¤º
        system_prompt = self._generate_system_prompt()
        
        # ç¡®å®šè¾“å‡ºæ–‡ä»¶è·¯å¾„
        if output_file is None:
            file_name = Path(file_path).stem
            output_file = f"{file_name}_dialogues.{Config.OUTPUT_FORMAT}"
        
        # å¤„ç†æ¯ä¸ªæ–‡æœ¬å—
        total_dialogues = 0
        
        with tqdm(total=len(chunks), desc="æå–å¯¹è¯", initial=processed_chunks) as pbar:
            for i, chunk in enumerate(chunks):
                if i < processed_chunks:
                    continue
                
                try:
                    # è°ƒç”¨APIæå–å¯¹è¯
                    response = self._call_api_with_retry(system_prompt, chunk)
                    dialogues = self._parse_and_validate_response(response)
                    
                    # å»é‡
                    unique_dialogues = self._remove_duplicates(dialogues)
                    
                    # è½¬æ¢ä¸ºå¸¦chunk-idçš„æ ¼å¼ï¼ˆå¦‚æœå¯ç”¨ï¼‰
                    if self.include_chunk_id:
                        chunk_dialogues = []
                        for dialogue_index, dialogue in enumerate(unique_dialogues):
                            chunk_dialogue = ChunkDialogueItem(
                                chunk_id=i,
                                dialogue_index=dialogue_index,
                                role=dialogue.role,
                                dialogue=dialogue.dialogue,
                                chunk_text=chunk if self.save_chunk_text else None
                            )
                            chunk_dialogues.append(chunk_dialogue)
                        
                        # ä¿å­˜ç»“æœ
                        with open(output_file, 'a', encoding=Config.OUTPUT_ENCODING) as f:
                            for chunk_dialogue in chunk_dialogues:
                                json.dump(chunk_dialogue.to_dict(include_chunk_text=self.save_chunk_text), f, ensure_ascii=False)
                                f.write('\n')
                    else:
                        # ä½¿ç”¨æ—§æ ¼å¼
                        with open(output_file, 'a', encoding=Config.OUTPUT_ENCODING) as f:
                            for dialogue in unique_dialogues:
                                json.dump(dialogue.to_dict(), f, ensure_ascii=False)
                                f.write('\n')
                    
                    total_dialogues += len(unique_dialogues)
                    
                    # ä¿å­˜è¿›åº¦
                    self._save_progress(file_path, i + 1, len(chunks))
                    
                    # æ›´æ–°è¿›åº¦æ¡
                    pbar.set_postfix({
                        'å¯¹è¯æ•°': total_dialogues,
                        'å»é‡å': len(unique_dialogues)
                    })
                    pbar.update(1)
                    
                except Exception as e:
                    logger.error(f"å¤„ç†ç¬¬ {i+1} ä¸ªå—æ—¶å‘ç”Ÿé”™è¯¯: {e}")
                    continue
        
        # æ¸…ç†è¿›åº¦æ–‡ä»¶
        progress_file = os.path.join(Config.CACHE_DIR, Config.PROGRESS_FILE)
        if os.path.exists(progress_file):
            try:
                os.remove(progress_file)
            except:
                pass
        
        logger.info(f"å¤„ç†å®Œæˆï¼å…±æå– {total_dialogues} æ¡å¯¹è¯ï¼Œä¿å­˜åˆ°: {output_file}")
        return output_file
    
    def extract_dialogues_concurrent(self, file_path: str, output_file: Optional[str] = None) -> str:
        """
        ä½¿ç”¨å¤šçº¿ç¨‹å¹¶å‘ä»æ–‡æœ¬æ–‡ä»¶ä¸­æå–å¯¹è¯
        
        Args:
            file_path: è¾“å…¥æ–‡æœ¬æ–‡ä»¶è·¯å¾„
            output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼Œå¦‚æœä¸ºNoneåˆ™è‡ªåŠ¨ç”Ÿæˆ
        
        Returns:
            è¾“å‡ºæ–‡ä»¶è·¯å¾„
        """
        logger.info(f"å¼€å§‹å¹¶å‘å¤„ç†æ–‡ä»¶: {file_path}")
        
        # è¯»å–æ–‡æœ¬æ–‡ä»¶
        text = self._read_text_file(file_path)
        
        # æ–‡æœ¬åˆ†å—
        chunks = self._chunk_text(text)
        logger.info(f"æ–‡æœ¬åˆ†å—å®Œæˆï¼šå…± {len(chunks)} ä¸ªå—ï¼Œå°†ä½¿ç”¨ {self.max_workers} ä¸ªçº¿ç¨‹å¹¶å‘å¤„ç†")
        
        # ç”Ÿæˆç³»ç»Ÿæç¤º
        system_prompt = self._generate_system_prompt()
        
        # ç¡®å®šè¾“å‡ºæ–‡ä»¶è·¯å¾„
        if output_file is None:
            file_name = Path(file_path).stem
            output_file = f"{file_name}_dialogues_concurrent.{Config.OUTPUT_FORMAT}"
        
        # åˆ›å»ºçº¿ç¨‹å®‰å…¨çš„æå–å™¨
        thread_safe_extractor = ThreadSafeDialogueExtractor(self, self.include_chunk_id)
        
        # å‡†å¤‡å·¥ä½œé˜Ÿåˆ—
        work_items = [
            WorkItem(index=i, chunk_id=i, chunk=chunk, system_prompt=system_prompt)
            for i, chunk in enumerate(chunks)
        ]
        
        # ä½¿ç”¨çº¿ç¨‹æ± å¹¶å‘å¤„ç†
        total_dialogues = 0
        failed_chunks = 0
        
        # ç”¨äºæŒ‰é¡ºåºä¿å­˜ç»“æœçš„ç¼“å†²åŒº
        results_buffer: Dict[int, List[Union[DialogueItem, ChunkDialogueItem]]] = {}
        completed_chunks = set()
        
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            # æäº¤æ‰€æœ‰ä»»åŠ¡
            future_to_item = {
                executor.submit(thread_safe_extractor.process_chunk, item): item
                for item in work_items
            }
            
            # ä½¿ç”¨è¿›åº¦æ¡æ˜¾ç¤ºè¿›åº¦
            with tqdm(total=len(work_items), desc="å¹¶å‘æå–å¯¹è¯") as pbar:
                for future in as_completed(future_to_item):
                    work_item = future_to_item[future]
                    
                    try:
                        # è·å–ç»“æœ
                        dialogues = future.result()
                        
                        # å°†ç»“æœå­˜å…¥ç¼“å†²åŒº
                        with thread_safe_extractor.lock:
                            results_buffer[work_item.chunk_id] = dialogues
                            completed_chunks.add(work_item.chunk_id)
                        
                        total_dialogues += len(dialogues)
                        
                        # æ£€æŸ¥æ˜¯å¦å¯ä»¥æŒ‰é¡ºåºå†™å…¥æ–‡ä»¶
                        self._write_ordered_results(results_buffer, completed_chunks, output_file)
                        
                        # æ›´æ–°è¿›åº¦æ¡
                        pbar.set_postfix({
                            'å¯¹è¯æ•°': total_dialogues,
                            'å¤±è´¥å—æ•°': failed_chunks,
                            'æ´»è·ƒçº¿ç¨‹': executor._work_queue.qsize()
                        })
                        pbar.update(1)
                        
                    except Exception as e:
                        failed_chunks += 1
                        logger.error(f"å¤„ç†ç¬¬ {work_item.index + 1} ä¸ªå—æ—¶å‘ç”Ÿé”™è¯¯: {e}")
                        pbar.set_postfix({
                            'å¯¹è¯æ•°': total_dialogues,
                            'å¤±è´¥å—æ•°': failed_chunks,
                            'æ´»è·ƒçº¿ç¨‹': executor._work_queue.qsize()
                        })
                        pbar.update(1)
        
        # å¤„ç†æ‰€æœ‰å‰©ä½™çš„ç»“æœ
        self._flush_remaining_results(results_buffer, output_file)
        
        # è¾“å‡ºé”™è¯¯æ±‡æ€»
        if thread_safe_extractor.errors:
            logger.warning(f"å¤„ç†è¿‡ç¨‹ä¸­å‘ç”Ÿ {len(thread_safe_extractor.errors)} ä¸ªé”™è¯¯")
            for error in thread_safe_extractor.errors[:5]:  # åªæ˜¾ç¤ºå‰5ä¸ªé”™è¯¯
                logger.warning(f"  - {error}")
            if len(thread_safe_extractor.errors) > 5:
                logger.warning(f"  - ... è¿˜æœ‰ {len(thread_safe_extractor.errors) - 5} ä¸ªé”™è¯¯")
        
        logger.info(f"å¹¶å‘å¤„ç†å®Œæˆï¼å…±æå– {total_dialogues} æ¡å¯¹è¯ï¼Œå¤±è´¥ {failed_chunks} ä¸ªå—ï¼Œä¿å­˜åˆ°: {output_file}")
        return output_file
    
    def _write_ordered_results(self, results_buffer: Dict[int, List], completed_chunks: Set[int], output_file: str):
        """æŒ‰é¡ºåºå†™å…¥ç»“æœåˆ°æ–‡ä»¶"""
        expected_chunk_id = len(results_buffer) - len(completed_chunks)
        
        while expected_chunk_id in results_buffer:
            dialogues = results_buffer.pop(expected_chunk_id)
            
            # å†™å…¥æ–‡ä»¶
            with open(output_file, 'a', encoding=Config.OUTPUT_ENCODING) as f:
                for dialogue in dialogues:
                    if isinstance(dialogue, ChunkDialogueItem):
                        json.dump(dialogue.to_dict(include_chunk_text=self.save_chunk_text), f, ensure_ascii=False)
                    else:
                        json.dump(dialogue.to_dict(), f, ensure_ascii=False)
                    f.write('\n')
            
            expected_chunk_id += 1
    
    def _flush_remaining_results(self, results_buffer: Dict[int, List], output_file: str):
        """åˆ·æ–°æ‰€æœ‰å‰©ä½™ç»“æœåˆ°æ–‡ä»¶"""
        if not results_buffer:
            return
        
        logger.info(f"åˆ·æ–°å‰©ä½™ {len(results_buffer)} ä¸ªå—çš„ç»“æœåˆ°æ–‡ä»¶")
        
        # æŒ‰chunk_idé¡ºåºå†™å…¥
        for chunk_id in sorted(results_buffer.keys()):
            dialogues = results_buffer[chunk_id]
            
            # å†™å…¥æ–‡ä»¶
            with open(output_file, 'a', encoding=Config.OUTPUT_ENCODING) as f:
                for dialogue in dialogues:
                    if isinstance(dialogue, ChunkDialogueItem):
                        json.dump(dialogue.to_dict(include_chunk_text=self.save_chunk_text), f, ensure_ascii=False)
                    else:
                        json.dump(dialogue.to_dict(), f, ensure_ascii=False)
                    f.write('\n')
    
    def get_statistics(self, output_file: str) -> Dict[str, Any]:
        """è·å–è¾“å‡ºæ–‡ä»¶çš„ç»Ÿè®¡ä¿¡æ¯"""
        try:
            dialogues = []
            with open(output_file, 'r', encoding='utf-8') as f:
                for line in f:
                    if line.strip():
                        data = json.loads(line.strip())
                        dialogues.append(data)
            
            # ç»Ÿè®¡è§’è‰²å¯¹è¯æ•°é‡
            role_counts = {}
            for dialogue in dialogues:
                role = dialogue.get('role', 'Unknown')
                role_counts[role] = role_counts.get(role, 0) + 1
            
            return {
                'total_dialogues': len(dialogues),
                'unique_roles': len(role_counts),
                'role_distribution': role_counts,
                'average_dialogue_length': sum(len(d.get('dialogue', '')) for d in dialogues) / len(dialogues) if dialogues else 0
            }
            
        except Exception as e:
            logger.error(f"ç»Ÿè®¡ä¿¡æ¯ç”Ÿæˆå¤±è´¥: {e}")
            return {}
    
    def sort_dialogues(self, output_file: str, sorted_output_file: Optional[str] = None) -> str:
        """æŒ‰chunk_idæ’åºå¯¹è¯å¹¶ä¿å­˜åˆ°æ–°æ–‡ä»¶"""
        if sorted_output_file is None:
            base_name = Path(output_file).stem
            sorted_output_file = f"{base_name}_sorted.{Config.OUTPUT_FORMAT}"
        
        try:
            # è¯»å–æ‰€æœ‰å¯¹è¯
            dialogues = []
            with open(output_file, 'r', encoding='utf-8') as f:
                for line in f:
                    if line.strip():
                        data = json.loads(line.strip())
                        dialogues.append(data)
            
            # æ£€æŸ¥æ˜¯å¦æœ‰chunk_id
            has_chunk_id = any('chunk_id' in d for d in dialogues)
            
            if has_chunk_id:
                # æŒ‰chunk_idå’Œdialogue_indexæ’åº
                dialogues.sort(key=lambda x: (x.get('chunk_id', 0), x.get('dialogue_index', 0)))
            else:
                logger.warning("æ–‡ä»¶ä¸­ä¸åŒ…å«chunk_idä¿¡æ¯ï¼Œæ— æ³•æ’åº")
                return output_file
            
            # å†™å…¥æ’åºåçš„æ–‡ä»¶
            with open(sorted_output_file, 'w', encoding=Config.OUTPUT_ENCODING) as f:
                for dialogue in dialogues:
                    json.dump(dialogue, f, ensure_ascii=False)
                    f.write('\n')
            
            logger.info(f"å¯¹è¯æ’åºå®Œæˆï¼Œä¿å­˜åˆ°: {sorted_output_file}")
            return sorted_output_file
            
        except Exception as e:
            logger.error(f"å¯¹è¯æ’åºå¤±è´¥: {e}")
            return output_file
    
    def filter_by_chunk(self, output_file: str, chunk_ids: List[int], filtered_output_file: Optional[str] = None) -> str:
        """æŒ‰chunk_idç­›é€‰å¯¹è¯å¹¶ä¿å­˜åˆ°æ–°æ–‡ä»¶"""
        if filtered_output_file is None:
            base_name = Path(output_file).stem
            chunk_str = '_'.join(map(str, sorted(chunk_ids)))
            filtered_output_file = f"{base_name}_chunks_{chunk_str}.{Config.OUTPUT_FORMAT}"
        
        try:
            filtered_dialogues = []
            with open(output_file, 'r', encoding='utf-8') as f:
                for line in f:
                    if line.strip():
                        data = json.loads(line.strip())
                        if data.get('chunk_id') in chunk_ids:
                            filtered_dialogues.append(data)
            
            # å†™å…¥ç­›é€‰åçš„æ–‡ä»¶
            with open(filtered_output_file, 'w', encoding=Config.OUTPUT_ENCODING) as f:
                for dialogue in filtered_dialogues:
                    json.dump(dialogue, f, ensure_ascii=False)
                    f.write('\n')
            
            logger.info(f"æŒ‰chunkç­›é€‰å®Œæˆï¼Œä¿å­˜åˆ°: {filtered_output_file} (ç­›é€‰äº† {len(filtered_dialogues)} æ¡å¯¹è¯)")
            return filtered_output_file
            
        except Exception as e:
            logger.error(f"æŒ‰chunkç­›é€‰å¤±è´¥: {e}")
            return output_file
    
    def get_chunk_statistics(self, output_file: str) -> Dict[str, Any]:
        """è·å–æŒ‰chunkåˆ†ç»„çš„ç»Ÿè®¡ä¿¡æ¯"""
        try:
            chunk_stats = {}
            total_dialogues = 0
            
            with open(output_file, 'r', encoding='utf-8') as f:
                for line in f:
                    if line.strip():
                        data = json.loads(line.strip())
                        chunk_id = data.get('chunk_id')
                        
                        if chunk_id is not None:
                            if chunk_id not in chunk_stats:
                                chunk_stats[chunk_id] = {
                                    'dialogue_count': 0,
                                    'roles': {},
                                    'total_length': 0
                                }
                            
                            chunk_stats[chunk_id]['dialogue_count'] += 1
                            role = data.get('role', 'Unknown')
                            chunk_stats[chunk_id]['roles'][role] = chunk_stats[chunk_id]['roles'].get(role, 0) + 1
                            chunk_stats[chunk_id]['total_length'] += len(data.get('dialogue', ''))
                            total_dialogues += 1
            
            # è®¡ç®—æ±‡æ€»ç»Ÿè®¡
            summary = {
                'total_chunks': len(chunk_stats),
                'total_dialogues': total_dialogues,
                'average_dialogues_per_chunk': total_dialogues / len(chunk_stats) if chunk_stats else 0,
                'chunk_details': chunk_stats
            }
            
            return summary
            
        except Exception as e:
            logger.error(f"Chunkç»Ÿè®¡ä¿¡æ¯ç”Ÿæˆå¤±è´¥: {e}")
            return {}
    
    def convert_to_legacy_format(self, output_file: str, legacy_output_file: Optional[str] = None) -> str:
        """è½¬æ¢ä¸ºæ—§æ ¼å¼ï¼ˆä¸å«chunk_idï¼‰ç”¨äºå‘åå…¼å®¹"""
        if legacy_output_file is None:
            base_name = Path(output_file).stem
            legacy_output_file = f"{base_name}_legacy.{Config.OUTPUT_FORMAT}"
        
        try:
            with open(legacy_output_file, 'w', encoding=Config.OUTPUT_ENCODING) as f_out:
                with open(output_file, 'r', encoding='utf-8') as f_in:
                    for line in f_in:
                        if line.strip():
                            data = json.loads(line.strip())
                            
                            # åˆ›å»ºæ—§æ ¼å¼æ•°æ®
                            legacy_data = {
                                'role': data.get('role', ''),
                                'dialogue': data.get('dialogue', '')
                            }
                            
                            json.dump(legacy_data, f_out, ensure_ascii=False)
                            f_out.write('\n')
            
            logger.info(f"è½¬æ¢ä¸ºæ—§æ ¼å¼å®Œæˆï¼Œä¿å­˜åˆ°: {legacy_output_file}")
            return legacy_output_file
            
        except Exception as e:
            logger.error(f"æ ¼å¼è½¬æ¢å¤±è´¥: {e}")
            return output_file

def main():
    """ä¸»å‡½æ•° - ç¤ºä¾‹ç”¨æ³•"""
    import argparse
    
    parser = argparse.ArgumentParser(description='ä»å°è¯´ä¸­æå–è§’è‰²å¯¹è¯')
    parser.add_argument('input_file', nargs='?', help='è¾“å…¥æ–‡æœ¬æ–‡ä»¶è·¯å¾„')
    parser.add_argument('-o', '--output', help='è¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼‰')
    parser.add_argument('--stats', action='store_true', default=Config.DEFAULT_SHOW_STATS, help='æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯ (é»˜è®¤: å¼€å¯)')
    parser.add_argument('--no-stats', action='store_false', dest='stats', help='ä¸æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯')
    parser.add_argument('-p', '--platform', help='æŒ‡å®šä½¿ç”¨çš„å¹³å° (å¦‚: deepseek, openai, moonshotç­‰)')
    parser.add_argument('-l', '--list-platforms', action='store_true', help='åˆ—å‡ºæ‰€æœ‰æ”¯æŒçš„å¹³å°')
    parser.add_argument('-t', '--threads', type=int, default=Config.MAX_WORKERS, help=f'æŒ‡å®šå¹¶å‘çº¿ç¨‹æ•° (é»˜è®¤: {Config.MAX_WORKERS})')
    parser.add_argument('--concurrent', action='store_true', default=Config.DEFAULT_CONCURRENT, help='ä½¿ç”¨å¤šçº¿ç¨‹å¹¶å‘å¤„ç† (é»˜è®¤: å¼€å¯)')
    parser.add_argument('--no-concurrent', action='store_false', dest='concurrent', help='ä½¿ç”¨å•çº¿ç¨‹å¤„ç†')
    parser.add_argument('--no-chunk-id', action='store_true', help='ä¸åœ¨è¾“å‡ºä¸­åŒ…å«chunk-idä¿¡æ¯')
    parser.add_argument('--save-chunk-text', action='store_true', default=Config.DEFAULT_SAVE_CHUNK_TEXT, help='ä¿å­˜åŸå§‹chunkæ–‡æœ¬ (é»˜è®¤: å¼€å¯)')
    parser.add_argument('--no-save-chunk-text', action='store_false', dest='save_chunk_text', help='ä¸ä¿å­˜åŸå§‹chunkæ–‡æœ¬')
    parser.add_argument('--sort-output', action='store_true', default=Config.DEFAULT_SORT_OUTPUT, help='å®ŒæˆåæŒ‰chunk_idæ’åºè¾“å‡ºæ–‡ä»¶ (é»˜è®¤: å¼€å¯)')
    parser.add_argument('--no-sort-output', action='store_false', dest='sort_output', help='ä¸æ’åºè¾“å‡ºæ–‡ä»¶')
    parser.add_argument('--legacy-format', action='store_true', help='åŒæ—¶ç”Ÿæˆæ—§æ ¼å¼æ–‡ä»¶ï¼ˆä¸å«chunk_idï¼‰')
    
    args = parser.parse_args()
    
    # åˆ—å‡ºæ”¯æŒçš„å¹³å°
    if args.list_platforms:
        from config import ModelPlatform
        print("=== æ”¯æŒçš„æ¨¡å‹å¹³å° ===")
        for name, description in ModelPlatform.list_platforms().items():
            print(f"  {name}: {description}")
        print(f"\nå½“å‰é»˜è®¤å¹³å°: {Config.CURRENT_PLATFORM}")
        return 0
    
    # æ£€æŸ¥æ˜¯å¦æä¾›äº†è¾“å…¥æ–‡ä»¶
    if not args.input_file:
        parser.error("è¯·æä¾›è¾“å…¥æ–‡ä»¶è·¯å¾„")
    
    try:
        # åˆ›å»ºæå–å™¨å®ä¾‹
        extractor = DialogueExtractor(
            platform=args.platform, 
            max_workers=args.threads,
            include_chunk_id=not args.no_chunk_id,
            save_chunk_text=args.save_chunk_text
        )
        
        # æå–å¯¹è¯
        if args.concurrent:
            print(f"ğŸš€ ä½¿ç”¨å¤šçº¿ç¨‹å¹¶å‘å¤„ç† ({extractor.max_workers} ä¸ªçº¿ç¨‹)")
            output_file = extractor.extract_dialogues_concurrent(args.input_file, args.output)
        else:
            print(f"ğŸ“ ä½¿ç”¨å•çº¿ç¨‹å¤„ç†")
            output_file = extractor.extract_dialogues(args.input_file, args.output)
        
        # åå¤„ç†ï¼šæ’åºè¾“å‡º
        if args.sort_output and extractor.include_chunk_id:
            print(f"ğŸ”„ æŒ‰chunk_idæ’åºè¾“å‡ºæ–‡ä»¶...")
            sorted_file = extractor.sort_dialogues(output_file)
            print(f"âœ… æ’åºå®Œæˆ: {sorted_file}")
            output_file = sorted_file
        
        # åå¤„ç†ï¼šç”Ÿæˆæ—§æ ¼å¼æ–‡ä»¶
        if args.legacy_format and extractor.include_chunk_id:
            print(f"ğŸ“„ ç”Ÿæˆæ—§æ ¼å¼æ–‡ä»¶...")
            legacy_file = extractor.convert_to_legacy_format(output_file)
            print(f"âœ… æ—§æ ¼å¼æ–‡ä»¶: {legacy_file}")
        
        # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
        if args.stats:
            stats = extractor.get_statistics(output_file)
            print(f"\n=== ç»Ÿè®¡ä¿¡æ¯ ===")
            print(f"ä½¿ç”¨å¹³å°: {extractor.platform}")
            print(f"ä½¿ç”¨æ¨¡å‹: {extractor.model_name}")
            if args.concurrent:
                print(f"å¤„ç†æ–¹å¼: å¤šçº¿ç¨‹å¹¶å‘ ({extractor.max_workers} ä¸ªçº¿ç¨‹)")
            else:
                print(f"å¤„ç†æ–¹å¼: å•çº¿ç¨‹")
            print(f"è¾“å‡ºæ ¼å¼: {'åŒ…å«chunk-id' if extractor.include_chunk_id else 'ä¸åŒ…å«chunk-id'}")
            print(f"æ€»å¯¹è¯æ•°: {stats['total_dialogues']}")
            print(f"è§’è‰²æ•°é‡: {stats['unique_roles']}")
            print(f"å¹³å‡å¯¹è¯é•¿åº¦: {stats['average_dialogue_length']:.1f} å­—ç¬¦")
            
            # å¦‚æœåŒ…å«chunk-idï¼Œæ˜¾ç¤ºchunkç»Ÿè®¡ä¿¡æ¯
            if extractor.include_chunk_id:
                chunk_stats = extractor.get_chunk_statistics(output_file)
                if chunk_stats:
                    print(f"æ€»å—æ•°: {chunk_stats['total_chunks']}")
                    print(f"å¹³å‡æ¯å—å¯¹è¯æ•°: {chunk_stats['average_dialogues_per_chunk']:.1f}")
                    
                    # æ˜¾ç¤ºå‰5ä¸ªæœ€æ´»è·ƒçš„chunk
                    if chunk_stats['chunk_details']:
                        print(f"\næœ€æ´»è·ƒçš„æ–‡æœ¬å—:")
                        sorted_chunks = sorted(chunk_stats['chunk_details'].items(), 
                                              key=lambda x: x[1]['dialogue_count'], reverse=True)[:5]
                        for chunk_id, details in sorted_chunks:
                            print(f"  å— {chunk_id}: {details['dialogue_count']} æ¡å¯¹è¯")
            
            print(f"\nè§’è‰²åˆ†å¸ƒ:")
            for role, count in sorted(stats['role_distribution'].items(), key=lambda x: x[1], reverse=True):
                print(f"  {role}: {count} æ¡")
        
    except Exception as e:
        logger.error(f"ç¨‹åºæ‰§è¡Œå¤±è´¥: {e}")
        return 1
    
    return 0

if __name__ == "__main__":
    exit(main())